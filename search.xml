<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>大数据入门09-HDFS总结</title>
      <link href="/post/2a6eff8e.html"/>
      <url>/post/2a6eff8e.html</url>
      
        <content type="html"><![CDATA[<p class="description"></p><img src="https://" alt="" style="width:100%" /><span id="more"></span><h2 id="HDFS概述"><a href="#HDFS概述" class="headerlink" title="HDFS概述"></a>HDFS概述</h2><p>HDFS（Hadoop Distributed File System），它是一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。<br>HDFS的使用场景：适合一次写入，多次读出的场景，且不支持文件的修改。适合用来做数据分析，并不适合用来做网盘应用。</p><p>###HDFS优缺点<br>####优点</p><ul><li><p>高容错性<br>1)数据自动保存多个副本。它通过增加副本的形式，提高容错性<br>2)某一个副本丢失以后，它可以自动恢复</p></li><li><p>适合处理大数据<br>1)数据规模：能够处理数据规模达到GB、TB、甚至PB级别的数据<br>2)文件规模：能够处理百万规模以上的文件数量，数量相当之大</p></li><li><p>可构建在廉价机器上，通过多副本机制，提高可靠性</p></li></ul><p>####缺点</p><ul><li><p>不适合低延时数据访问，比如毫秒级的存储数据，是做不到的</p></li><li><p>无法高效的对大量小文件进行存储<br>1)存储大量小文件的话，它会占用NameNode大量的内存来存储文件目录和块信息。这样是不可取的，因为NameNode的内存总是有限的<br>2)小文件存储的寻址时间会超过读取时间，它违反了HDFS的设计目标</p></li><li><p>不支持并发写入、文件随机修改<br>1)一个文件只能有一个写，不允许多个线程同时写<br>2)仅支持数据append（追加），不支持文件的随机修改</p></li></ul><p>###HDFS组成架构</p><ol><li>NameNode（nn）：就是Master，它是一个主管、管理者。</li></ol><ul><li>管理HDFS的名称空间</li><li>配置副本策略</li><li>管理数据块（Block）映射信息</li><li>处理客户端读写请求</li></ul><ol start="2"><li>DataNode：就是Slave。NameNode下达命令，DataNode执行实际的操作</li></ol><ul><li>存储实际的数据块</li><li>执行数据块的读&#x2F;写操作</li></ul><ol start="3"><li>Client：就是客户端</li></ol><ul><li>文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行上传</li><li>与NameNode交互，获取文件的位置信息</li><li>与DataNode交互，读取或者写入数据</li><li>Client提供一些命令来管理HDFS，比如NameNode格式化</li><li>Client可以通过一些命令来访问HDFS，比如对HDFS增删查改操作</li></ul><ol start="4"><li>Secondary NameNode：并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务</li></ol><ul><li>辅助NameNode，分担其工作量，比如定期合并Fsimage和Edits，并推送给NameNode </li><li>在紧急情况下，可辅助恢复NameNode</li></ul><p>##HDFS的常用Shell操作</p><p>###基本语法<br>bin&#x2F;hadoop fs 具体命令   OR  bin&#x2F;hdfs dfs 具体命令<br>dfs是fs的实现类</p><p>###命令大全</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">[-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">        [-cat [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">        [-checksum &lt;src&gt; ...]</span><br><span class="line">        [-chgrp [-R] GROUP PATH...]</span><br><span class="line">        [-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]</span><br><span class="line">        [-chown [-R] [OWNER][:[GROUP]] PATH...]</span><br><span class="line">        [-copyFromLocal [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">        [-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">        [-count [-q] &lt;path&gt; ...]</span><br><span class="line">        [-cp [-f] [-p] &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">        [-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]]</span><br><span class="line">        [-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;]</span><br><span class="line">        [-df [-h] [&lt;path&gt; ...]]</span><br><span class="line">        [-du [-s] [-h] &lt;path&gt; ...]</span><br><span class="line">        [-expunge]</span><br><span class="line">        [-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">        [-getfacl [-R] &lt;path&gt;]</span><br><span class="line">        [-getmerge [-nl] &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">        [-help [cmd ...]]</span><br><span class="line">        [-ls [-d] [-h] [-R] [&lt;path&gt; ...]]</span><br><span class="line">        [-mkdir [-p] &lt;path&gt; ...]</span><br><span class="line">        [-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">        [-moveToLocal &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">        [-mv &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">        [-put [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">        [-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;]</span><br><span class="line">        [-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...]</span><br><span class="line">        [-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...]</span><br><span class="line">        [-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--set &lt;acl_spec&gt; &lt;path&gt;]]</span><br><span class="line">        [-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...]</span><br><span class="line">        [-stat [format] &lt;path&gt; ...]</span><br><span class="line">        [-tail [-f] &lt;file&gt;]</span><br><span class="line">        [-test -[defsz] &lt;path&gt;]</span><br><span class="line">        [-text [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">        [-touchz &lt;path&gt; ...]</span><br><span class="line">        [-usage [cmd ...]]</span><br></pre></td></tr></table></figure><p>###常用命令实操</p><ul><li><p>-help：输出这个命令参数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -help rm</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ... :</span><br><span class="line">  Delete all files that match the specified file pattern. Equivalent to the Unix</span><br><span class="line">  command &quot;rm &lt;src&gt;&quot;</span><br><span class="line">                                                                                 </span><br><span class="line">  -skipTrash  option bypasses trash, if enabled, and immediately deletes &lt;src&gt;   </span><br><span class="line">  -f          If the file does not exist, do not display a diagnostic message or </span><br><span class="line">              modify the exit status to reflect an error.                        </span><br><span class="line">  -[rR]       Recursively deletes directories  </span><br></pre></td></tr></table></figure></li><li><p>-ls: 显示目录信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -ls /</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Found 5 items</span><br><span class="line">-rw-r--r--   3 root supergroup       1366 2019-02-15 15:33 /README.txt</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2019-02-21 14:20 /directory</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2019-02-18 10:09 /hadoop</span><br><span class="line">drwxrwxr-x   - root supergroup          0 2019-02-20 16:24 /tmp</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2019-02-27 11:48 /user</span><br></pre></td></tr></table></figure></li><li><p>-mkdir：在HDFS上创建目录</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir -p /sanguo/shuguo</span><br></pre></td></tr></table></figure><p><img src="https://dik111-1258101294.cos.ap-guangzhou.myqcloud.com/20190416174323.png"></p></li><li><p>-moveFromLocal：从本地剪切粘贴到HDFS</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">touch kongming.txt</span><br><span class="line">hadoop fs -moveFromLocal ./kongming.txt /sanguo/shuguo</span><br></pre></td></tr></table></figure><p><img src="https://dik111-1258101294.cos.ap-guangzhou.myqcloud.com/20190416174603.png"></p></li><li><p>-appendToFile：追加一个文件到已经存在的文件末尾</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">touch liubei.txt</span><br><span class="line">vim liubei.txt</span><br><span class="line">hadoop fs -appendToFile liubei.txt /sanguo/shuguo/kongming.txt</span><br></pre></td></tr></table></figure></li><li><p>-cat：显示文件内容</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -cat /sanguo/shuguo/kongming.txt</span><br></pre></td></tr></table></figure><p><img src="https://dik111-1258101294.cos.ap-guangzhou.myqcloud.com/20190416175122.png"></p></li><li><p>-chgrp 、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs  -chmod  666  /sanguo/shuguo/kongming.txt</span><br><span class="line">hadoop fs  -chown  atguigu:atguigu   /sanguo/shuguo/kongming.txt</span><br></pre></td></tr></table></figure></li><li><p>-copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -copyFromLocal NOTICE.txt /</span><br></pre></td></tr></table></figure><p><img src="https://dik111-1258101294.cos.ap-guangzhou.myqcloud.com/20190416175646.png"></p></li><li><p>-copyToLocal：从HDFS拷贝到本地</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -copyToLocal /sanguo/shuguo/kongming.txt ./</span><br></pre></td></tr></table></figure></li><li><p>-cp ：从HDFS的一个路径拷贝到HDFS的另一个路径</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -cp /sanguo/shuguo/kongming.txt /zhuge.txt</span><br></pre></td></tr></table></figure><p><img src="https://dik111-1258101294.cos.ap-guangzhou.myqcloud.com/20190416181455.png"></p></li><li><p>-mv：在HDFS目录中移动文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mv /zhuge.txt /sanguo/shuguo/</span><br></pre></td></tr></table></figure><p><img src="https://dik111-1258101294.cos.ap-guangzhou.myqcloud.com/20190416181749.png"></p></li><li><p>-getmerge：合并下载多个文件，比如HDFS的目录 &#x2F;user&#x2F;atguigu&#x2F;test下有多个文件:log.1, log.2,log.3,…</p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -getmerge /sanguo/shuguo/* ./zaiyiqi.txt</span><br></pre></td></tr></table></figure><ul><li><p>-get：等同于copyToLocal，就是从HDFS下载文件到本地</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -get /sanguo/shuguo/kongming.txt ./</span><br></pre></td></tr></table></figure></li><li><p>-put：等同于copyFromLocal</p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -put ./zaiyiqi.txt /user/</span><br></pre></td></tr></table></figure><ul><li><p>-rm：删除文件或文件夹</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -rm /user/zaiyiqi.txt</span><br></pre></td></tr></table></figure></li><li><p>-du统计文件夹的大小信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -du -s -h /user</span><br></pre></td></tr></table></figure><p><img src="https://dik111-1258101294.cos.ap-guangzhou.myqcloud.com/20190416182500.png"></p></li></ul><h2 id="HDFS的数据流"><a href="#HDFS的数据流" class="headerlink" title="HDFS的数据流"></a>HDFS的数据流</h2><h3 id="HDFS写数据流程"><a href="#HDFS写数据流程" class="headerlink" title="HDFS写数据流程"></a>HDFS写数据流程</h3><p><img src="https://dik111-1258101294.cos.ap-guangzhou.myqcloud.com/20190417152653.png"></p><ul><li>客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在</li><li>NameNode返回是否可以上传</li><li>客户端请求第一个 Block上传到哪几个DataNode服务器上</li><li>NameNode返回3个DataNode节点，分别为dn1,dn2,dn3</li><li>客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成</li><li>dn1、dn2、dn3逐级应答客户端</li><li>客户端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答</li><li>当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。（重复执行3-7步）</li></ul><h3 id="HDFS读数据流程"><a href="#HDFS读数据流程" class="headerlink" title="HDFS读数据流程"></a>HDFS读数据流程</h3><p><img src="https://dik111-1258101294.cos.ap-guangzhou.myqcloud.com/20190423163351.png"></p><ul><li>客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址</li><li>挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据</li><li>DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）</li><li>客户端以Packet为单位接收，先在本地缓存，然后写入目标文件</li></ul><p>##SecondaryNameNode和NameNode</p><p><img src="https://dik111-1258101294.cos.ap-guangzhou.myqcloud.com/20190423173551.png"></p><p>###第一阶段：NameNode启动</p><ul><li>第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存</li><li>客户端对元数据进行增删改的请求</li><li>NameNode记录操作日志，更新滚动日志</li><li>NameNode在内存中对数据进行增删改</li></ul><h3 id="第二阶段：Secondary-NameNode工作"><a href="#第二阶段：Secondary-NameNode工作" class="headerlink" title="第二阶段：Secondary NameNode工作"></a>第二阶段：Secondary NameNode工作</h3><ul><li>Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果</li><li>Secondary NameNode请求执行CheckPoint</li><li>NameNode滚动正在写的Edits日志</li><li>将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode</li><li>Secondary NameNode加载编辑日志和镜像文件到内存，并合并</li><li>生成新的镜像文件fsimage.chkpoint</li><li>拷贝fsimage.chkpoint到NameNode</li><li>NameNode将fsimage.chkpoint重新命名成fsimage</li></ul><h2 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h2><h3 id="DataNode工作机制"><a href="#DataNode工作机制" class="headerlink" title="DataNode工作机制"></a>DataNode工作机制</h3><p><img src="https://dik111-1258101294.cos.ap-guangzhou.myqcloud.com/20190423181910.png"></p><ul><li>一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳</li><li>DataNode启动后向NameNode注册，通过后，周期性（1小时）的向NameNode上报所有的块信息</li><li>心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用</li><li>集群运行中可以安全加入和退出一些机器<hr /></li></ul>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> HDFS </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
